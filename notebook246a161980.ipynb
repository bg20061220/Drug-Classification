{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1419436,"sourceType":"datasetVersion","datasetId":830916}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.stats import shapiro\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import GridSearchCV \n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('/kaggle/input/drug-classification/drug200.csv')\ndf.head()\n# Checking Unique Values in Bp and Cholestrol for encodig them \ndf['BP'].unique()  # All Values - High Low Normal \ndf['Cholesterol'].unique() # High , Normal\n# Encoding Bp and cholestrol \ndf['BP'] = df['BP'].map({'HIGH' : 1 , 'LOW' : -1 , 'NORMAL' : 0})\ndf.head()\ndf['Cholesterol'] = df['Cholesterol'].map({'HIGH' : 1 , 'NORMAL' : -1})\ndf.head()\n# Performing Statistical Tests to determine wheather to perform Standardization or Normalization on Data\n# Test -1 : Shapiro Wilk Test \ndata = df['Na_to_K'].dropna()\nstat , p = shapiro(data)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# stat = 0.902 and p = 0.000 suggesting data is not normally distributed \n# Drawing a histogram to see if the data is normally distrubuted \n#plt.figure(figsize=(10, 6))  \n#plt.hist(data, bins=15, color='blue', alpha=0.7)  \n#plt.title('Histogram of Na_to_K Name')  \n#plt.xlabel('Value')  \n#plt.ylabel('Frequency')  \n#plt.grid(axis='y', alpha=0.75)  \n#plt.show() \n# Data is left skewed , will do normalization on the Na_to_K column\nmin_value = df['Na_to_K'].min()\nmax_value = df['Na_to_K'].max()\ndf['Normalized_Na_to_K'] = (df['Na_to_K'] - min_value) / (max_value - min_value)\n\n# Encoding Values in the drug column and sex column\ndf['Drug'].unique()\ndf['Drug'] = df['Drug'].map({'DrugY' : 0 , 'drugC' : 1 , 'drugX' : 2 , 'drugA' : 3 , 'drugB' : 4 })\ndf.head()\ndf['Sex'].unique()\ndf['Sex'] = df['Sex'].map({'F' : 0 , 'M' : 1})\n\n# Using Multinomial Logisitic Regression to make predictions \n# Making Feature and target columns\nX = df[['Age' , 'Sex' , 'BP' , 'Cholesterol', 'Normalized_Na_to_K']]\ny = df[['Drug']]\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n# Tuning the hyperparameter for best results \nparam_grid = {\n    'C' : [0.001 , 0.01 , 0.1  , 1 , 10 , 100],\n    'solver': ['lbfgs' , 'saga'],\n    'max_iter' : [100 , 200 , 300] , \n    'penalty' : ['l1' , 'l2'],\n    'multi_class' : ['ovr' , 'multinomial']\n}\n# Initialising the Model \nmodel = LogisticRegression(\n    C = 100,\n    max_iter = 100, \n    multi_class = 'ovr',\n    penalty = 'l2',\n    solver = 'lbfgs'\n)\nmodel.fit(X_train , y_train)\ny_pred = model.predict(X_test)\nprint(f\"Best Results : {y_pred}\")\naccuracy_score = accuracy_score(y_pred , y_test)\nprint(f\"ACCURACY SCORE : {accuracy_score}\")\n# Accuracy Score = 97.5% \n# Performing the Grid Search \n#grid_search = GridSearchCV(model , param_grid , cv = 5 , scoring= 'accuracy', error_score=np.nan)\n# Fitting data into grid-search\n#grid_search.fit(X_train , y_train)\n#print(\"Best hyperparameters:\", grid_search.best_params_) #Best hyperparameters: {'C': 100, 'max_iter': 100, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'lbfgs'}\n# Making Prediction and checking accuracy score \n#best_model = grid_search.best_estimator_ \n#y_pred = best_model.predict(X_test)\n#print(f\"Best Results : {y_pred} \")\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-12T01:33:45.586466Z","iopub.execute_input":"2024-10-12T01:33:45.587629Z","iopub.status.idle":"2024-10-12T01:33:45.688640Z","shell.execute_reply.started":"2024-10-12T01:33:45.587581Z","shell.execute_reply":"2024-10-12T01:33:45.687434Z"}},"outputs":[],"execution_count":null}]}